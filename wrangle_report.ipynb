{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: wragle_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of wrangling twitter data was iterative. Going from gathering, assessing, cleaning, sorting, and ultimately analyzing and visualizing the data brought many interesting insights. After gathering the data through the documents provided, reading the data into dataframes took place. Each dataset required different processes for ensuring the data was clean and useful. For example, the csv provided was simple to read however, the tsv required a clarification of the which delimeter to use. Lastly, the tweet-json.txt file contained several JSON objects which took the most effort to extract. First the txt file needed to be read, formatted into JSON, and lastly read into a dataframe as a csv. \n",
    "\n",
    "After this process was completed, the three dataframes required a visual and programmatic assesment. This was done to ensure the data didn't contain any errors that would give inaccurate results. This process took serveral steps, establishing which issues had to do with the quality of data itself and which issues had to do with tidiness. Once the quality issues were resolved, the tidiness issues were simple to resolve as it was simply dealing with restructuring the dataframes. First the three dataframes were merged into one, then two additional columns were created to prepare for the analysis portion of the notebook. \n",
    "\n",
    "After the data was assessed and cleaned, it was then stored into a csv file. That csv file was read into a dataframe and used for the analysis portion of our notebook. In analyzing the data, three questions were asked. \n",
    "\n",
    "1. What are the top 10 most popular dog names in our dataset?\n",
    "2. What breed of dog is tweeted about the most?\n",
    "3. Does a higher rating mean a larger amount of retweets?\n",
    "\n",
    "These questions helped determine what columns to use. They also were used to determine how to explore the final dataframe. Once the proper code was implented, the questions were answered as noted below.\n",
    "\n",
    "#### The top ten most popular dog names in our dataset are:<br>\n",
    "\n",
    "\n",
    "1. Lucy\n",
    "2. Charlie\n",
    "3. Cooper\n",
    "4. Oliver\n",
    "5. Tucker\n",
    "6. Penny\n",
    "7. Lola\n",
    "8. Sadie\n",
    "9. Winston\n",
    "10. Toby\n",
    "\n",
    "\n",
    "#### The top ten most popular dog breeds in our dataset are:\n",
    "\n",
    "1. Golden Retriever       \n",
    "2. Labrador Retriever      \n",
    "3. Pembroke              \n",
    "4. Chihuahua              \n",
    "5. Pug                     \n",
    "6. Chow                   \n",
    "7. Pomeranian              \n",
    "8. Toy Poodle              \n",
    "9. Samoyed                \n",
    "10. Malamute               \n",
    "\n",
    "\n",
    "\n",
    "#### The higher the rating for a dog does not necessarily equal more retweets:\n",
    "\n",
    "Although one dog recieved a rating of 177.6, it didn't get the most retweets. The rating with the most retweets by a large margin is 0.64 with a rating of 0.82 coming in with the second most retweets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
